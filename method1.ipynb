{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"method1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m9Vn5DId4CWu","colab_type":"text"},"source":["##Training Model"]},{"cell_type":"code","metadata":{"id":"y4a8XSln_Ibu","colab_type":"code","outputId":"acbc791a-3519-46ee-a8d6-ac8e81423eec","executionInfo":{"status":"ok","timestamp":1574820672922,"user_tz":-480,"elapsed":1824,"user":{"displayName":"葉加祈","photoUrl":"","userId":"16275266334170874501"}},"colab":{"base_uri":"https://localhost:8080/","height":465}},"source":["import os\n","import random\n","import glob\n","import numpy as np\n","import pandas as pd\n","import torch\n","import csv\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.optim import Adam\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","from torchsummary import summary\n","import face_recognition\n","\n","Base_dir = 'data/'\n","\n","class Resnet18(nn.Module):\n","    def __init__(self):\n","        super(Resnet18, self).__init__()\n","        self.resnet = nn.Sequential(*list(models.resnet18(pretrained=True).children())[:-1])\n","        self.fc = nn.Linear(512,7)\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = x.view(-1, 1*1*512)\n","        x = self.fc(x)\n","        \n","        return x\n","\n","class hw3_Testdataset(Dataset):\n","    \n","    def __init__(self, data, transform):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.data[idx]).convert('RGB')\n","        img = self.transform(img)\n","        return img\n","\n","def getPredict(test_path, model_path):\n","\n","  transform = transforms.Compose([transforms.ToTensor()])\n","\n","  test_img = sorted(glob.glob(os.path.join(test_path, '*.jpg')))\n","  test_dataset = hw3_Testdataset(test_img,transform)\n","  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","\n","  model = Resnet18()\n","  model.load_state_dict(torch.load(model_path))\n","  device = torch.device('cuda')\n","  model = model.to(device)\n","  model.eval()\n","  \n","  prediction = []\n","  with torch.no_grad():\n","    for img in test_loader:\n","      img = img.to(device)\n","      out = model(img)\n","      _, pred_label = torch.max(out, 1)\n","      for i in range(pred_label.size(0)):\n","        prediction.append(pred_label[i].item())\n","\n","  df = pd.DataFrame({'id': np.arange(0,len(prediction)), 'label': prediction})\n","  df.to_csv(Base_dir+'prediction_2.csv',index=False)\n","  \n","  return prediction\n","\n","def getLandmark(test_path):\n","  \n","  lands = []\n","  test_img = sorted(glob.glob(os.path.join(test_path, '*.jpg')))\n","\n","  for i, name in enumerate(test_img):\n","\n","  # Load the jpg file into a numpy array\n","    image = face_recognition.load_image_file(name)\n","\n","    # Find all facial features in all the faces in the image\n","    face_landmarks_list = face_recognition.face_landmarks(image)\n","\n","    #Create a white image to show landmark only\n","    landmark_img = Image.new('RGB', (48,48), (255,255,255,0))\n","    pure_d = ImageDraw.Draw(landmark_img)\n","\n","    for face_landmarks in face_landmarks_list:\n","\n","        for facial_feature in face_landmarks.keys():\n","            pure_d.line(face_landmarks[facial_feature], width=1, fill='black')\n","\n","    landmark_img.save(Base_dir+'Test_landmark/landmark_{}.jpg'.format(i))\n","    print('{} landmark file number saved'.format(i))\n","    \n","    land = np.zeros(1)\n","    for val in face_landmarks.values():\n","      nVal = np.array(val).flatten()\n","      land = np.append(land, nVal)\n","    land = np.delete(land,0)\n","    lands.append(land)\n","\n","  lands = np.array(lands)\n","  np.save(Base_dir+'Test_landmarks.npy', lands)\n","  print('Test landmarks shape: ',lands.shape)\n","  \n","  return lands\n","\n","def mixLandmark(emo, ori):\n","  \n","  response = np.genfromtxt(Base_dir+'prediction_2.csv', delimiter=',', dtype=int, skip_header=1)[:,1]\n","  test_land = np.load(Base_dir+'Test_landmarks.npy')\n","  basic_land = np.load(Base_dir+'remix_landmark/remix_landmark.npy')\n","  print('basic_land shape: ' , basic_land.shape)\n","  \n","  emotion_land = np.zeros((1,144))\n","  for res in response:\n","    emotion_land = np.concatenate((emotion_land, basic_land[res].reshape((1,144))))\n","  emotion_land = np.delete(emotion_land, 0, 0)\n","  print('emotion landmarks shape: ', emotion_land.shape) # suppose to be (21,144)\n","\n","  # get the mean landmark\n","  print('test landmarks shape: ', test_land.shape)\n","  mix_land = np.around(emotion_land*emo + test_land*ori)\n","  mix_land = np.array(mix_land, dtype=int)\n","\n","  # Output the Image\n","  lines = [17, 5, 5, 4, 5, 6, 6, 12, 12]\n","  ret = []\n","  for idx in range(21):\n","\n","    landmark_img = Image.new('RGB', (48,48), (255,255,255,0))\n","    pure_d = ImageDraw.Draw(landmark_img)\n","\n","\n","    base = 0\n","    for dotnum in lines:\n","      mark = []\n","      for k in range(dotnum):\n","        ptr = base + k*2\n","        point = (mix_land[idx][ptr], mix_land[idx][ptr+1])\n","        mark.append(point)\n","      pure_d.line(mark, width=1, fill='black')\n","      base += dotnum*2\n","\n","    #landmark_img.save(Base_dir+'emo_remix/result_{}.jpg'.format(idx))\n","\n","    #im1 = Image.open(Base_dir+'Testdata/{}.jpg'.format(str(idx).zfill(2))).convert('RGB')\n","    #im2 = landmark_img\n","\n","    #combo = Image.fromarray(np.hstack((np.array(im1),np.array(im2))))\n","\n","    #combo.show()\n","    #combo.save(Base_dir+'result_landmark/result_landmark_{}.jpg'.format(idx))\n","\n","    ret.append(landmark_img)\n","  \n","  return ret\n","\n","\n","#prediction = getPredict(Base_dir+'Testdata', Base_dir+'Imgemo_model.pth')\n","#test_landmarks = getLandmark(Base_dir+'Testdata')\n","mixLandmark(0.8,0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["basic_land shape:  (7, 144)\n","emotion landmarks shape:  (21, 144)\n","test landmarks shape:  (21, 144)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8EB8>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8C50>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8DA0>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8C88>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8E10>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8CC0>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8B00>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8A58>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD89E8>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD88D0>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8BE0>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8EF0>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8F28>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BD8F98>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE048>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE0B8>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE128>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE198>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE208>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE278>,\n"," <PIL.Image.Image image mode=RGB size=48x48 at 0x7FB6B1BDE2E8>]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Qd08cM73ni2C","colab_type":"code","outputId":"27ee1a46-4f23-40af-bb75-6ff78b0fb639","executionInfo":{"status":"ok","timestamp":1574820678495,"user_tz":-480,"elapsed":2402,"user":{"displayName":"葉加祈","photoUrl":"","userId":"16275266334170874501"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["ret1 = mixLandmark(0.6, 0.4)\n","ret2 = mixLandmark(0.8, 0.2)\n","\n","for i in range(21):\n","    im1 = Image.open(Base_dir+'Testdata/{}.jpg'.format(str(i).zfill(2))).convert('RGB')\n","    im2 = ret1[i]\n","    im3 = ret2[i]\n","    images = [im1, im2, im3]\n","    \n","    total_width = 48*3\n","    max_height = 48\n","\n","    new_im = Image.new('RGB', (total_width, max_height))\n","\n","    x_offset = 0\n","    for im in images:\n","        new_im.paste(im, (x_offset,0))\n","        x_offset += im.size[0]\n","\n","    new_im.save(Base_dir+'method2_img2emotion/comp/result_{}.jpg'.format(i))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["basic_land shape:  (7, 144)\n","emotion landmarks shape:  (21, 144)\n","test landmarks shape:  (21, 144)\n","basic_land shape:  (7, 144)\n","emotion landmarks shape:  (21, 144)\n","test landmarks shape:  (21, 144)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ed8WwWEhHKdU","colab_type":"code","colab":{}},"source":["for i in range(21):\n","    im1 = Image.open(Base_dir+'Testdata/{}.jpg'.format(str(i).zfill(2))).convert('RGB')\n","    im2 = Image.open(Base_dir+'emo_remix/result_{}.jpg'.format(i))\n","    im3 = Image.open(Base_dir+'response_basic/result_{}.jpg'.format(i))\n","    images = [im1, im2, im3]\n","    \n","    total_width = 48*3\n","    max_height = 48\n","\n","    new_im = Image.new('RGB', (total_width, max_height))\n","\n","    x_offset = 0\n","    for im in images:\n","        new_im.paste(im, (x_offset,0))\n","        x_offset += im.size[0]\n","\n","    new_im.save(Base_dir+'final_result/result_{}.jpg'.format(i))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-Ch3bfg_vg_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktTj8iMbBa-A","colab_type":"code","colab":{}},"source":["!pip install face_recognition"],"execution_count":0,"outputs":[]}]}